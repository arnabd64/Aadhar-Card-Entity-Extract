{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNJqO3hnbog1OJJx2xXLja4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnabd64/Aadhar-Card-Entity-Extract/blob/main/Langchain_Day_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain Document Retrieval Query\n",
        "\n",
        "In this notebook we will build a chatbot that is capable of Retrieval Augmented Generation or RAG. It is a process where we feed the Large Language Modelwith data it has not seen during it's training process and ask questions based off that unseen data.\n",
        "\n",
        "There are a lot of components involved in building the chain and we will be covering only the important ones.\n",
        "\n",
        "1. Document Loaders\n",
        "2. Text Splitter\n",
        "3. Embeddings & Document Embeddings\n",
        "4. Vector Store\n",
        "5. Retriever"
      ],
      "metadata": {
        "id": "iOx6MX7Q-O61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ],
      "metadata": {
        "id": "1xPzj8AD_2ht"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mqmxZS1-o2XI"
      },
      "outputs": [],
      "source": [
        "! pip install --progress-bar=off --no-cache-dir \\\n",
        "    langchain==0.2.10 \\\n",
        "    langchain-community==0.2.10 \\\n",
        "    langchain-chroma \\\n",
        "    langchain-text-splitters \\\n",
        "    chromadb \\\n",
        "    pypdf \\\n",
        "    python-dotenv \\\n",
        "> install.log"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import dotenv\n",
        "assert dotenv.load_dotenv('./.env'), 'Unable to load ./.env'"
      ],
      "metadata": {
        "id": "ubkzWdZNpLQ0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Components"
      ],
      "metadata": {
        "id": "qnbQU6Wm_6gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.chat_message_histories.file import FileChatMessageHistory\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings.ollama import OllamaEmbeddings\n",
        "from langchain_community.chat_models.ollama import ChatOllama\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "import chromadb"
      ],
      "metadata": {
        "id": "RsgtxkTZpLTt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Document Loader"
      ],
      "metadata": {
        "id": "qR3ltCVCADIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A document loader is langchain module that helps to load and process documents in langchain. There are several document loaders ranging from PDF, Plaintext, Marekdown, HTML Webpages and more. A `Document` according to langchain is a piece of text along with optional metadata.\n",
        "\n",
        "Langchain Documentation: [Document Loaders](https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/)"
      ],
      "metadata": {
        "id": "W7J-q70MAGPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are going to query from the famous 2017 Academic paper [Attention is All you need](https://arxiv.org/pdf/1706.03762). You can download the PDF from the link or can run the folloiwng command in Google Colab:\n",
        "\n",
        "```bash\n",
        "wget -O Attention-is-all-you-need.pdf https://arxiv.org/pdf/1706.03762\n",
        "```"
      ],
      "metadata": {
        "id": "hKfKY4s0CKOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_file = PyPDFLoader('/content/Attention-is-all-you-need.pdf')"
      ],
      "metadata": {
        "id": "Vdd9xobFDmeR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Text Splitter\n",
        "\n",
        "One problem with Large Language Models is that if we feed an entire document to them then there arises two issues, First higher computation times due to the large amounts of text sent as input and second which is that the input text is longer than the model's context window which results in the model hallucinating.\n",
        "\n",
        "The solution to this is issue is to split the document into smaller chunks and instead of feeding the entire document to the LLM, we only feed the chunks that contain relevant information needed to answer the user's question.\n",
        "\n",
        "Langchain Documentation: [Text Splitter](https://python.langchain.com/v0.2/docs/concepts/#text-splitters)"
      ],
      "metadata": {
        "id": "UcBaHqDDETST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the `RecursiveCharacterTextSplitter` which allows the user to choose the character separators. Each chunk will have $256$ characters in length and $16$ chartacters of overlap. The overlap is used so that a chunk can contain text from the previous chunk. The number of characters to be included from the previous chunk is specified by the `chunk_overlap` parameter."
      ],
      "metadata": {
        "id": "3rf9lR8xRE1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=16)\n",
        "documents = pdf_file.load_and_split(text_splitter)\n",
        "\n",
        "print(f\"Total Chunks: {len(documents)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpsmNtGipLbZ",
        "outputId": "d6ad1ade-92b4-4633-ab20-3552b28fee84"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Chunks: 193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Embeddings\n",
        "\n",
        "Once the document has been splitted into smaller chunks, it is time to generate embeddings. _Embeddings are a vector representation of a text_.\n",
        "\n",
        "1. For each chunk of text, an embedding is generated using an embeddings model.\n",
        "2. All the text chunk and their corresponding embedding is stored as a key value pair in an __Embedding Store__ or __Vector Store__\n",
        "3. An embedding is generated for the user's query.\n",
        "4. A similarity search is performed between the user's query and text chunks based off their embeddings where each texty chunk is assigned a similarity score.\n",
        "5. All chunks that score below a threshold are rejected and the top $k$ text chunks are used as a context for the LLM input.\n",
        "\n",
        "Langchain Documentation: [Embeddings Model](https://python.langchain.com/v0.2/docs/concepts/#embedding-models)\n",
        "\n",
        "Ollama provides embedding model: [`nomic-embed-text`](https://ollama.com/library/nomic-embed-text) we will be using this model to generate our embeddings. To download the model run `ollama pull nomic-embed-text`"
      ],
      "metadata": {
        "id": "mUvqkz4sT6r2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = OllamaEmbeddings(\n",
        "    base_url = os.getenv('HOST'),\n",
        "    model = os.getenv('EMBED')\n",
        ")"
      ],
      "metadata": {
        "id": "AJKa4_5RXxYC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Vector Store\n",
        "\n",
        "A vector store is a database that stores the pairs of text chunks and their embeddings. Langchain provides quite a variety of vector stores ranging from local in memory vector stores to  vector databases hosted on the cloud.\n",
        "\n",
        "Langchain Documentation: [Vector Stores](https://python.langchain.com/v0.2/docs/concepts/#vector-stores)\n",
        "\n",
        "We will be using [Chroma](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/) DB with persistent storage, which means that all the key value pairs will be storage in the local storage as `sqlite3` database."
      ],
      "metadata": {
        "id": "IUcM3PUJX7Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DOCUMENT_STORE_NAME = 'my_documents'\n",
        "\n",
        "# create the vector store\n",
        "vector_store = Chroma(\n",
        "    collection_name = DOCUMENT_STORE_NAME,\n",
        "    client = chromadb.PersistentClient(path=DOCUMENT_STORE_NAME)\n",
        ")\n",
        "\n",
        "# add documets\n",
        "vector_store = vector_store.from_documents(documents, embedding_model)"
      ],
      "metadata": {
        "id": "pBAAruWzpLgO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Retriever\n",
        "\n",
        "A retriever as the name suggests retrieves relevant documents from the vector store. it is tasked with performing a similarity search and assigning a score to each text chunk using their embedding.\n",
        "\n",
        "Langchain Documentation: [Retriever](https://python.langchain.com/v0.2/docs/concepts/#retrievers)"
      ],
      "metadata": {
        "id": "OdFnTV-HIVAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_settings = {\n",
        "    'search_type': 'mmr',\n",
        "    'search_kwargs': {\n",
        "        'k': 5,\n",
        "        'score_threshold': 0.2\n",
        "    }\n",
        "}\n",
        "retriever = vector_store.as_retriever(**search_settings)"
      ],
      "metadata": {
        "id": "rc5APAojuMSb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conversational Chain\n",
        "\n",
        "Once the document has been loaded, splitted, embedded and stored in a vector store, it is time to build the chain that will carry out the conversation with the documents and the LLM. The chain will have basic conversation capabilities along with chat mmessage history which will be stored as a JSON file."
      ],
      "metadata": {
        "id": "SNtsgQmLJnyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initialize Large Language Model"
      ],
      "metadata": {
        "id": "KLQOqqaC8mMM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_llm = ChatOllama(\n",
        "    base_url = os.getenv('HOST'),\n",
        "    model = os.getenv('LLM'),\n",
        "    temperature = 0.8,\n",
        "    timeout = 600,\n",
        "    keep_alive = 3600\n",
        ")"
      ],
      "metadata": {
        "id": "OaoC0UQsR--n"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. History Aware Retriever\n",
        "\n",
        "First we need a chain that takes in the user's query and previous chat history and based off that information generates a query that will be used for similarity search on the vector store."
      ],
      "metadata": {
        "id": "VudUJoBA8sp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain to contextualize the question\n",
        "system_prompt_question_contextualize_template = \"\"\"\n",
        "Given a chat history and the latest user's question which might reference context\n",
        "from the chat history. Formulate a standalone question which can be understood\n",
        "without the chat history. Do NOT answer the question, just formulate it if needed\n",
        "and otherwise return it as it.\n",
        "\"\"\"\n",
        "contextualize_prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', system_prompt_question_contextualize_template.replace('\\n','')),\n",
        "    ('placeholder', '{chat_history}'),\n",
        "    ('human', '{input}')\n",
        "])\n",
        "\n",
        "history_aware_retriever = create_history_aware_retriever(chat_llm, retriever, contextualize_prompt)"
      ],
      "metadata": {
        "id": "hxnRIhR667D9"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Question Answer chain\n",
        "\n",
        "The QA chain is our main chain which deals with answering the user's questions using the documents from the previous chain as an input."
      ],
      "metadata": {
        "id": "3iHDU8NA_4P3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chain to answer the question\n",
        "system_prompt = \"\"\"\n",
        "You are an assistant who is tasked for question answering tasks.\n",
        "Use the following pieces of retrieved context to answer the question.\n",
        "\\m\\n\n",
        "{context}\n",
        "\"\"\"\n",
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    ('system', system_prompt),\\\n",
        "    ('placeholder', '{chat_history}'),\n",
        "    ('human', '{input}')\n",
        "])\n",
        "\n",
        "combine_documents = create_stuff_documents_chain(chat_llm, qa_prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, combine_documents)"
      ],
      "metadata": {
        "id": "qRHXJ56vTMjh"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Add chat memory to the chain\n",
        "\n",
        "We will be using `FileChatMessageHistory` module that will store the chat history as a `JSON` file."
      ],
      "metadata": {
        "id": "_YFvyTApBg7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chat_history(session_id: str):\n",
        "    filename = f\"{session_id}.json\"\n",
        "    return FileChatMessageHistory(filename, encoding='utf-8')"
      ],
      "metadata": {
        "id": "n7dj42yXWuWL"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversational_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_chat_history,\n",
        "    input_messages_key='input',\n",
        "    history_messages_key='chat_history',\n",
        "    output_messages_key='answer'\n",
        ")\n",
        "\n",
        "conversational_chain = (\n",
        "    {'input': RunnablePassthrough()}\n",
        "    | conversational_chain\n",
        "    | RunnableLambda(lambda x: x['answer'])\n",
        ")"
      ],
      "metadata": {
        "id": "uAPsHudvVDY8"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the Final Chain"
      ],
      "metadata": {
        "id": "yK-0W3n0CCci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = 'Explain me attention mechanism?'"
      ],
      "metadata": {
        "id": "HQdZROj_ZEpu"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = conversational_chain.invoke(\n",
        "    question,\n",
        "    config={'configurable': {'session_id': '3481'}}\n",
        ")"
      ],
      "metadata": {
        "id": "899y0p_IVU_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "sxu6x9J-ZKyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}